{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# _Automate Documentation, Comments, and Unit Tests for Python Code_  \n",
        "# _Convert Python to Optimized C++ Code_\n",
        "\n",
        "## Overview  \n",
        "It is a Gradio-powered tool designed to automate essential but time-consuming Python development tasks. It streamlines documentation, unit testing, and Python-to-C++ code conversion with AI-driven assistance.  \n",
        "\n",
        "### Key Features  \n",
        "**Auto-Generate Docstrings & Comments** – Instantly improve code clarity and maintainability.  \n",
        "**Unit Test Generation** – Ensure reliability with AI-generated test cases.  \n",
        "**Python to C++ Conversion** – Seamlessly translate Python code to C++ with execution support.  \n",
        "\n",
        "### Model used\n",
        "1. ***Frontier model***- gpt-4o-mini\n",
        "2. ***Open source model*** - meta-llama/Meta-Llama-3-8B-Instruct"
      ],
      "metadata": {
        "id": "IEdZqGuEqM2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keVpj9HgvQKb",
        "outputId": "47780ac7-5794-4f5e-a92f-b6814cc8988b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai gradio huggingface_hub transformers bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wpECjYVGwQRm"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TextStreamer\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dCDi7rtfyfQG"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1GQlxyj0y6pY"
      },
      "outputs": [],
      "source": [
        "gpt = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XjrZDqF9z0-J"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "LLAMA = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "MODEL_LIST = [\"GPT\", \"LLAMA\"]\n",
        "\n",
        "TASK_COMMENT = \"commenting\"\n",
        "TASK_TEST = \"testing\"\n",
        "TASK_CONVERT = \"converting\"\n",
        "\n",
        "PYTHON_SCRIPT_DEFAULT = \"\"\"\n",
        "import time\n",
        "\n",
        "def calculate(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations+1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1/j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1/j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate(100_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oAI-d4oU0z-1"
      },
      "outputs": [],
      "source": [
        "#### System prompts\n",
        "\n",
        "# System prompts for commenting\n",
        "SYSTEM_PROMPT_COMMENTS = \"\"\"\n",
        "You are an AI model specializing in enhancing Python code documentation.\n",
        "Generate detailed and precise docstrings and inline comments for the provided Python code.\n",
        "Ensure the docstrings clearly describe the purpose, parameters, and return values of each function.\n",
        "Inline comments should explain complex or non-obvious code segments.\n",
        "Do not include any introductions, explanations, conclusions, or additional context.\n",
        "Return only the updated Python code enclosed within ```python ... ``` for proper formatting and syntax highlighting.\n",
        "\"\"\"\n",
        "\n",
        "# System prompts for testing\n",
        "SYSTEM_PROMPT_TESTS = \"\"\"\n",
        "You are an AI model specializing in generating comprehensive unit tests for Python code.\n",
        "Create Python unit tests that thoroughly validate the functionality of the given code.\n",
        "Use the `unittest` framework and ensure edge cases and error conditions are tested.\n",
        "Do not include any comments, introductions, explanations, conclusions, or additional context.\n",
        "Return only the unit test code enclosed within ```python ... ``` for proper formatting and syntax highlighting.\n",
        "\"\"\"\n",
        "\n",
        "# System prompts for code cenversion\n",
        "SYSTEM_PROMPT_CONVERT = \"\"\"\n",
        "You are an AI model specializing in high-performance code translation.\n",
        "Translate the given Python code into equivalent, optimized C++ code.\n",
        "Focus on:\n",
        "- Using efficient data structures and algorithms.\n",
        "- Avoiding unnecessary memory allocations and computational overhead.\n",
        "- Ensuring minimal risk of integer overflow by using appropriate data types.\n",
        "- Leveraging the C++ Standard Library (e.g., `<vector>`, `<algorithm>`) for performance and readability.\n",
        "Produce concise and efficient C++ code that matches the functionality of the original Python code.\n",
        "Do not include any comments, introductions, explanations, conclusions, or additional context..\n",
        "Return only the C++ code enclosed within ```cpp ... ``` for proper formatting and syntax highlighting.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M4HwCeXI2YUt"
      },
      "outputs": [],
      "source": [
        "### User prompts\n",
        "\n",
        "# User prompts for commenting\n",
        "def user_prompt_comments(python_code):\n",
        "  user_prompt = f\"\"\"\n",
        "Add detailed docstrings and inline comments to the following Python code:\n",
        "```python\n",
        "{python_code}\n",
        "```\n",
        "\"\"\"\n",
        "  return user_prompt\n",
        "\n",
        "# User prompts for testing\n",
        "def user_prompt_tests(python_code):\n",
        "    user_prompt = f\"\"\"\n",
        "Generate unit tests for the following Python code using the `unittest` framework:\n",
        "\n",
        "```python\n",
        "{python_code}\n",
        "```\n",
        "\"\"\"\n",
        "    return user_prompt\n",
        "\n",
        "# User prompts for code converting\n",
        "def user_prompt_convert(python_code):\n",
        "    user_prompt = f\"\"\"\n",
        "Convert the following Python code into C++:\n",
        "\n",
        "```python\n",
        "{python_code}\n",
        "```\n",
        "\"\"\"\n",
        "    return user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jnINQfeB3rVl"
      },
      "outputs": [],
      "source": [
        "# Function for calling OpenAI model\n",
        "\n",
        "def stream_gpt(system_prompt, user_prompt):\n",
        "  print(\"Inside GPT\")\n",
        "  stream = gpt.chat.completions.create(\n",
        "      model=OPENAI_MODEL,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": system_prompt},\n",
        "          {\"role\": \"user\", \"content\": user_prompt}\n",
        "      ],\n",
        "      stream=True\n",
        "  )\n",
        "\n",
        "  reply = \"\"\n",
        "  for chunk in stream:\n",
        "    reply += chunk.choices[0].delta.content or \"\"\n",
        "    yield reply.replace(\"```python\\n\", \"\").replace(\"```cpp\\n\", \"\").replace(\"```\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QTTTCU2UKAEd"
      },
      "outputs": [],
      "source": [
        "# Quantization Config - this allows us to load the model into memory and use less memory\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLAMA model\n",
        "\n",
        "model_llama = AutoModelForCausalLM.from_pretrained(\n",
        "    LLAMA,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "62b0ec0ca4f44debb2ad2ef1cc2e13e4",
            "5b9e525fed2a472ab8f54f9111f199f0",
            "310399a024914f999e64d0033cb7763e",
            "3efd39d1d4af40a4bdbf5b74e68aa259",
            "73a0f59d24a44e20a8efad36340183e1",
            "a588e39eec18469097cc9629e5442b85",
            "f49f5f414c964188990bec7b1763560e",
            "885b31aad4014f9591ac50e1570f966e",
            "c6babf70451c4cbf8651038d6b064648",
            "033d02de74a447ef91330a28d30f9f07",
            "47701ef406bd4cfc8b3d7682b7c525b1"
          ]
        },
        "id": "A_CGxgcCkn33",
        "outputId": "9096c09c-530c-40a8-dc73-89f5317af2c2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62b0ec0ca4f44debb2ad2ef1cc2e13e4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for calling open source LLAMA model\n",
        "\n",
        "def stream_llama(system_prompt, user_prompt):\n",
        "  print(\"Inside LLAMA\")\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "  reply = \"\"\n",
        "  for token in model_llama.generate(inputs, max_new_tokens=3000):\n",
        "      decoded_text = tokenizer.decode(token, skip_special_tokens=True)\n",
        "      reply += decoded_text\n",
        "      yield reply.replace(\"```python\\n\", \"\").replace(\"```cpp\\n\", \"\").replace(\"```\", \"\")"
      ],
      "metadata": {
        "id": "3m5-d-V2fA_i"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "VN_urIW66LPR"
      },
      "outputs": [],
      "source": [
        "# Function for setting prompts for user specific tasks\n",
        "\n",
        "def set_prompts(user_input, task):\n",
        "\n",
        "  task = task.lower()\n",
        "  if task == TASK_COMMENT.lower():\n",
        "    system_prompt = SYSTEM_PROMPT_COMMENTS\n",
        "    user_prompt = user_prompt_comments(user_input)\n",
        "  elif task == TASK_TEST.lower():\n",
        "    system_prompt = SYSTEM_PROMPT_TESTS\n",
        "    user_prompt = user_prompt_tests(user_input)\n",
        "  elif task == TASK_CONVERT.lower():\n",
        "    system_prompt = SYSTEM_PROMPT_CONVERT\n",
        "    user_prompt = user_prompt_convert(user_input)\n",
        "  else:\n",
        "    return None, None\n",
        "\n",
        "  return system_prompt, user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "V3E18dGBWK_S"
      },
      "outputs": [],
      "source": [
        "# Function for streaming response depending on the selected model\n",
        "\n",
        "def stream_respone(user_input, task, model):\n",
        "\n",
        "  system_prompt, user_prompt = set_prompts(user_input, task)\n",
        "  if system_prompt == None or user_prompt == None:\n",
        "    raise ValueError(\"Invalid task\")\n",
        "\n",
        "  if model == \"GPT\":\n",
        "    yield from stream_gpt(system_prompt, user_prompt)\n",
        "  elif model == \"LLAMA\":\n",
        "    yield from stream_llama(system_prompt, user_prompt)\n",
        "  else:\n",
        "    raise ValueError(\"Invalid model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "r7JqcnI4YYsI"
      },
      "outputs": [],
      "source": [
        "# Function for generating comments\n",
        "def generate_comments(python_code, selected_model):\n",
        "\n",
        "  yield from stream_respone(python_code, TASK_COMMENT, selected_model)\n",
        "  return\n",
        "\n",
        "# Function for testing\n",
        "def generate_tests(python_code, selected_model):\n",
        "\n",
        "  yield from stream_respone(python_code, TASK_TEST, selected_model)\n",
        "  return\n",
        "\n",
        "# Function for code conversion\n",
        "def convert_code(python_code, selected_model):\n",
        "\n",
        "  yield from stream_respone(python_code, TASK_CONVERT, selected_model)\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "WSaWcx0u4A1K"
      },
      "outputs": [],
      "source": [
        "css = \"\"\"\n",
        ".python {\n",
        "    background-color: #377ef0;\n",
        "    color: #ffffff;\n",
        "    padding: 0.5em;\n",
        "    border-radius: 5px; /* Slightly rounded corners */\n",
        "}\n",
        ".response {\n",
        "    background-color: #00549e;\n",
        "    color: #ffffff;\n",
        "    padding: 0.5em;\n",
        "    border-radius: 5px;\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "SC7b6vXtvrVG",
        "outputId": "87d38481-753d-4a22-82ee-80e094f19459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://3be07d42d2a4db6c9e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3be07d42d2a4db6c9e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3be07d42d2a4db6c9e.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# Create UI with Gradio\n",
        "\n",
        "with gr.Blocks() as ui:\n",
        "  with gr.Row():\n",
        "    python_code = gr.Textbox(label=\"Python code:\", value=PYTHON_SCRIPT_DEFAULT, lines=10, elem_classes=[\"python\"])\n",
        "    response = gr.Textbox(label=\"LLM Response:\", lines=10, elem_classes=[\"response\"])\n",
        "\n",
        "  with gr.Row():\n",
        "    model = gr.Dropdown([\"GPT\", \"LLAMA\"], label=\"Select model\", value=\"GPT\")\n",
        "  with gr.Row():\n",
        "    comment_button = gr.Button(\"Comment Code\")\n",
        "    tests_button = gr.Button(\"Create Test Cases\")\n",
        "    convert_button = gr.Button(\"Convert to C++\")\n",
        "\n",
        "  comment_button.click(generate_comments, inputs=[python_code, model], outputs=[response])\n",
        "  tests_button.click(generate_tests, inputs=[python_code, model], outputs=[response])\n",
        "  convert_button.click(convert_code, inputs=[python_code, model], outputs=[response])\n",
        "\n",
        "  ui.launch(inbrowser=True, share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up\n",
        "\n",
        "del model_llama\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "konL4eo2gbC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62b0ec0ca4f44debb2ad2ef1cc2e13e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b9e525fed2a472ab8f54f9111f199f0",
              "IPY_MODEL_310399a024914f999e64d0033cb7763e",
              "IPY_MODEL_3efd39d1d4af40a4bdbf5b74e68aa259"
            ],
            "layout": "IPY_MODEL_73a0f59d24a44e20a8efad36340183e1"
          }
        },
        "5b9e525fed2a472ab8f54f9111f199f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a588e39eec18469097cc9629e5442b85",
            "placeholder": "​",
            "style": "IPY_MODEL_f49f5f414c964188990bec7b1763560e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "310399a024914f999e64d0033cb7763e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_885b31aad4014f9591ac50e1570f966e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6babf70451c4cbf8651038d6b064648",
            "value": 4
          }
        },
        "3efd39d1d4af40a4bdbf5b74e68aa259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_033d02de74a447ef91330a28d30f9f07",
            "placeholder": "​",
            "style": "IPY_MODEL_47701ef406bd4cfc8b3d7682b7c525b1",
            "value": " 4/4 [01:10&lt;00:00, 15.21s/it]"
          }
        },
        "73a0f59d24a44e20a8efad36340183e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a588e39eec18469097cc9629e5442b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49f5f414c964188990bec7b1763560e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "885b31aad4014f9591ac50e1570f966e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6babf70451c4cbf8651038d6b064648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "033d02de74a447ef91330a28d30f9f07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47701ef406bd4cfc8b3d7682b7c525b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}